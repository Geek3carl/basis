{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LLM Chain（LLM链）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = OpenAI()\n",
    "prompt_tpl = PromptTemplate.from_template(\n",
    "    '请给我讲1个关于{type}的笑话'\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_tpl)\n",
    "print(chain.invoke('程序员'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "prompt_tpl = PromptTemplate.from_template(\n",
    "    '请给我讲1个关于{type}的笑话'\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm, prompt=prompt_tpl,\n",
    "    output_key='content' # 默认为 text\n",
    ")\n",
    "print(chain.invoke('程序员'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "prompt_tpl = PromptTemplate.from_template(\n",
    "    '请给我讲{count}个关于{type}的笑话'\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_tpl)\n",
    "print(chain.invoke({'count': 2, 'type': '程序员'}))\n",
    "print(chain.predict(count=1, type='程序员'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "prompt_tpl = PromptTemplate.from_template(\n",
    "    '请给我讲1个关于{type}的笑话'\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_tpl)\n",
    "inputs = [\n",
    "    {'type': '老师'},\n",
    "    {'type': '动物'},\n",
    "    {'type': '植物'},\n",
    "]\n",
    "res = chain.apply(inputs)\n",
    "for info in res:\n",
    "    print(info)\n",
    "\n",
    "print('='*20)\n",
    "# batch 还支持设置最大并发数 max_concurrency\n",
    "# 如果希望最大并发为5，则可以写为：\n",
    "# res = chain.batch(inputs, config={\"max_concurrency\": 5})\n",
    "res = chain.batch(inputs)\n",
    "for info in res:\n",
    "    print(info)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "instructions = output_parser.get_format_instructions()\n",
    "\n",
    "llm = OpenAI()\n",
    "prompt_tpl = PromptTemplate(\n",
    "    template='请给我举例3个最具有代表性的{type}名称\\n{instructions}',\n",
    "    input_variables=['type'],\n",
    "    partial_variables={'instructions': instructions}\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_tpl,\n",
    "    output_parser=output_parser\n",
    ")\n",
    "print(chain.invoke('编程语言'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, load_chain\n",
    "\n",
    "prompt_tpl = PromptTemplate.from_template(\n",
    "    '请给我讲1个关于{type}的笑话'\n",
    ")\n",
    "\n",
    "llm = OpenAI()\n",
    "chain = LLMChain(llm=llm, prompt=prompt_tpl)\n",
    "# 将chain保存到本地，支持 json 和 yaml 格式\n",
    "chain.save('chain.json')\n",
    "\n",
    "# 加载本地保存的chain文件\n",
    "new_chain = load_chain('chain.json')\n",
    "print(chain.invoke('程序员'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sequential Chain（顺序链）\n",
    "## SimpleSequentialChain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "script_prompt_tpl = PromptTemplate.from_template(\n",
    "    '你是一个优秀的编剧。请使用你丰富的想象力根据我定的标题编写一个故事概要。'\n",
    "    '标题:{title}'\n",
    ")\n",
    "\n",
    "script_llm = OpenAI(\n",
    "    model_name='gpt-3.5-turbo-instruct',\n",
    "    temperature=0.9,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "script_chain = LLMChain(llm=script_llm, prompt=script_prompt_tpl)\n",
    "\n",
    "# 创建广告链\n",
    "adv_prompt_tpl = PromptTemplate.from_template(\n",
    "    '你是一个优秀的广告写手。请根据我定的故事概要，'\n",
    "    '为我的故事写一段尽可能简短但要让人有观看欲望的广告词。'\n",
    "    '故事概要:{story}'\n",
    ")\n",
    "adv_llm = OpenAI(\n",
    "    model_name='gpt-3.5-turbo-instruct',\n",
    "    temperature=0.6,\n",
    "    max_tokens=2048\n",
    ")\n",
    "adv_chain = LLMChain(llm=adv_llm, prompt=adv_prompt_tpl)\n",
    "\n",
    "# 将剧本链和广告链串联起来\n",
    "chain = SimpleSequentialChain(\n",
    "    chains=[script_chain, adv_chain],\n",
    "    verbose=True\n",
    ")\n",
    "print(chain.invoke('孙悟空大战变型金刚'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SequentialChain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "# 创建剧本链\n",
    "script_prompt_tpl = PromptTemplate.from_template(\n",
    "    '你是一个优秀的编剧。请使用你丰富的想象力根据我定的信息编写一个故事概要。'\n",
    "    '标题:{title}\\n故事类型:{story_type}\\n主角名:{name}'\n",
    ")\n",
    "\n",
    "script_llm = OpenAI(\n",
    "    model_name='gpt-3.5-turbo-instruct',\n",
    "    temperature=0.9,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "script_chain = LLMChain(\n",
    "    llm=script_llm,\n",
    "    prompt=script_prompt_tpl,\n",
    "    output_key='story'\n",
    ")\n",
    "\n",
    "# 创建广告链\n",
    "adv_prompt_tpl = PromptTemplate.from_template(\n",
    "    '你是一个优秀的广告词写手。请根据我给定的故事概要，'\n",
    "    '为我的故事写一段简短但要让人有观看欲望的广告词。'\n",
    "    '故事标题:{title}\\n'\n",
    "    '故事概要:{story}\\n'\n",
    "    '广告词:'\n",
    ")\n",
    "adv_llm = OpenAI(\n",
    "    model_name='gpt-3.5-turbo-instruct',\n",
    "    temperature=0.6,\n",
    "    max_tokens=1024\n",
    ")\n",
    "adv_chain = LLMChain(\n",
    "    llm=adv_llm,\n",
    "    prompt=adv_prompt_tpl,\n",
    "    output_key='adv'\n",
    ")\n",
    "\n",
    "# 将剧本链和广告链串联起来\n",
    "chain = SequentialChain(\n",
    "    chains=[script_chain, adv_chain],\n",
    "    input_variables=['title', 'story_type', 'name'],\n",
    "    # 如果希望输出script_chain的结果\n",
    "    # 这里可以设置为['story', 'adv']\n",
    "    output_variables=['adv']\n",
    ")\n",
    "print(chain.invoke(\n",
    "    {'title': '上海滩的秘密', 'story_type': '玄幻修真', 'name': '叶豪'}\n",
    "))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RouterChain（路由链）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.router.llm_router import (\n",
    "    LLMRouterChain,\n",
    "    RouterOutputParser\n",
    ")\n",
    "from langchain.chains.router.multi_prompt_prompt import (\n",
    "    MULTI_PROMPT_ROUTER_TEMPLATE\n",
    ")\n",
    "\n",
    "physics_prompt_tpl = PromptTemplate.from_template(\n",
    "    '你是个优秀的物理学家，你很擅长用简明易懂的方式回答有关物理学的问题。'\n",
    "    '请使用英文帮我解答下列问题：\\n{input}'\n",
    ")\n",
    "math_prompt_tpl = PromptTemplate.from_template(\n",
    "    '你是个很好的数学家。你很擅长回答数学问题。'\n",
    "    '请使用英文帮我解答下列问题：\\n{input}'\n",
    ")\n",
    "\n",
    "# 创建模板信息列表\n",
    "prompt_infos = [\n",
    "    {\n",
    "        'name': 'physics',\n",
    "        'description': '用于解答物理相关问题',\n",
    "        'prompt_template': physics_prompt_tpl,\n",
    "    },\n",
    "    {\n",
    "        'name': 'math',\n",
    "        'description': '用于解答数学相关问题',\n",
    "        'prompt_template': math_prompt_tpl,\n",
    "    },\n",
    "]\n",
    "llm = OpenAI(\n",
    "    model_name='gpt-3.5-turbo-instruct',\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# 生成键为模板名称、值为Chain的字典\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info['name']\n",
    "    prompt = p_info['prompt_template']\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "# 将模板名称和模板描述通过MULTI_PROMPT_ROUTER_TEMPLATE生成模板\n",
    "destinations = [f'{p[\"name\"]}: {p[\"description\"]}'\n",
    "                for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=['input'],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "# 这里创建了一个default_chain\n",
    "# 为了防止提的问题类型并没有包含在prompt_infos中\n",
    "default_chain = ConversationChain(llm=llm, output_key='text')\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What is Newton's First Law?\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains.router.embedding_router import EmbeddingRouterChain\n",
    "\n",
    "names_and_descriptions = [\n",
    "    ('physics', ['用于解答物理相关问题']),\n",
    "    ('math', [\"用于解答数学相关问题\"]),\n",
    "]\n",
    "\n",
    "router_chain = EmbeddingRouterChain.from_names_and_descriptions(\n",
    "    names_and_descriptions,\n",
    "    Chroma,\n",
    "    OpenAIEmbeddings(),\n",
    "    routing_keys=['input']\n",
    ")\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(chain.invoke('在物理学科中，牛顿第一定律是什么？'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TransformChain（转换链）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import (\n",
    "    TransformChain,\n",
    "    LLMChain,\n",
    "    SimpleSequentialChain\n",
    ")\n",
    "\n",
    "\n",
    "# 用于获取输入的前三段内容\n",
    "def transform_func(inputs):\n",
    "    text = inputs['text']\n",
    "    shortened_text = '\\n\\n'.join(text.split('\\n\\n')[:3])\n",
    "    return {'output_text': shortened_text}\n",
    "\n",
    "\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=['text'],\n",
    "    output_variables=['output_text'],\n",
    "    transform=transform_func\n",
    ")\n",
    "\n",
    "# 用户总结TransformChain发过来的数据\n",
    "template = \"\"\"Summarize this text:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "Summary:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['output_text'],\n",
    "    template=template\n",
    ")\n",
    "llm_chain = LLMChain(llm=OpenAI(), prompt=prompt)\n",
    "\n",
    "# 串联transform_chain、llm_chai\n",
    "sequential_chain = SimpleSequentialChain(\n",
    "    chains=[transform_chain, llm_chain]\n",
    ")\n",
    "with open('story.txt') as f:\n",
    "    sequential_chain.invoke(f.read())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SumarizeChain（总结链）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# 加载并分割文档\n",
    "loader = TextLoader('chat.txt')\n",
    "spliter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "documents = spliter.split_documents(loader.load())[:5]\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name='gpt-3.5-turbo-instruct',\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# 初始化SumarizeChain\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    ")\n",
    "print(chain.invoke(documents))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# APIChain与LLMRequestsChain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## APIChain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import APIChain\n",
    "from langchain.chains.api import open_meteo_docs\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "chain = APIChain.from_llm_and_api_docs(\n",
    "    llm=llm,\n",
    "    api_docs=open_meteo_docs.OPEN_METEO_DOCS,\n",
    "    limit_to_domains=['https://api.open-meteo.com/'],\n",
    ")\n",
    "print(chain.invoke('2024-07-25北京是多少度？'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LLMRequestsChain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMRequestsChain, LLMChain\n",
    "\n",
    "template = \"\"\"Between >>> and <<< are the raw search result text from bing.\n",
    "Extract the answer to the question '{query}' or say 'not found'\n",
    "if the information is not contained.\n",
    "Use the format\n",
    "Extracted:<answer or 'not found'>\n",
    ">>> {requests_result} <<<\n",
    "Extracted:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate.from_template(template)\n",
    "chain = LLMRequestsChain(\n",
    "    llm_chain=LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT),\n",
    ")\n",
    "question = 'On what day will the Hangzhou Asian Games be held?'\n",
    "inputs = {\n",
    "    'query': question,\n",
    "    'url': ('https://www.bing.com/search?form=&q=' +\n",
    "            question.replace(' ', '+')),\n",
    "}\n",
    "print(chain.invoke(inputs))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SQL Chain（数据库链）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SQLDatabaseChain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "\n",
    "llm = OpenAI()\n",
    "db = SQLDatabase.from_uri('sqlite:///students.sqlite')\n",
    "chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\n",
    "\n",
    "print(chain.invoke(\"What is the age of xiaoli?\"))\n",
    "print(chain.invoke(\"What is the phone number of xiaoli's father\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SQL Agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.agent_toolkits.sql.base import create_sql_agent\n",
    "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n",
    "\n",
    "db = SQLDatabase.from_uri('sqlite:///students1.sqlite')\n",
    "llm = OpenAI()\n",
    "executor = create_sql_agent(\n",
    "    llm=llm,\n",
    "    toolkit=SQLDatabaseToolkit(db=db, llm=llm),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(executor.invoke('Delete all contents in the info table'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# QA Chain（问答链）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ConversationChain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "conversation = ConversationChain(llm=chat)\n",
    "print(conversation.invoke('你是谁？'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RetrievalQA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "texts = [\n",
    "    '在这里！',\n",
    "    '你好啊！',\n",
    "    '你叫什么名字？',\n",
    "    '我的朋友们都叫我小明',\n",
    "    '哦，那太棒了！'\n",
    "]\n",
    "\n",
    "# 存储数据\n",
    "db = Chroma.from_texts(\n",
    "    texts, OpenAIEmbeddings(),\n",
    "    persist_directory='./chroma_test/'\n",
    ")\n",
    "\n",
    "# 创建问答链\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    retriever=db.as_retriever(),\n",
    ")\n",
    "print(qa_chain.invoke('你叫什么名字？'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ConversationalRetrievalChain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "texts = [\n",
    "    '在这里！',\n",
    "    '我的年龄是20岁',\n",
    "    '你叫什么名字？',\n",
    "    '我的朋友们都叫我小明',\n",
    "    '哦，那太棒了！'\n",
    "]\n",
    "\n",
    "# 存储数据\n",
    "db = Chroma.from_texts(\n",
    "    texts, OpenAIEmbeddings(),\n",
    "    persist_directory='./chroma_test/'\n",
    ")\n",
    "\n",
    "# 创建问答链\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    OpenAI(temperature=0),\n",
    "    db.as_retriever()\n",
    ")\n",
    "chat_history = []\n",
    "# 开始问答\n",
    "# chat_history为必传参数，因为默认提示模板会用到\n",
    "res = qa_chain.invoke({\n",
    "    \"question\": '你叫什么名字',\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "# 将返回值存入chat_history，当作下次提问的提示词的一部分传给LLM\n",
    "chat_history.append((res['question'], res['answer']))\n",
    "\n",
    "print(qa_chain.invoke({\n",
    "    \"question\": '你今年几岁了',\n",
    "    \"chat_history\": chat_history\n",
    "}))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LangChain Expression Language（LCEL）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 管道操作符"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_tpl = PromptTemplate.from_template(\n",
    "    '请给我讲1个关于{type}的笑话'\n",
    ")\n",
    "\n",
    "chain = prompt_tpl | OpenAI()\n",
    "print(chain.invoke({'type': '程序员'}))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 在链中设置参数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers.openai_functions import (\n",
    "    JsonOutputFunctionsParser\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    '请告诉我，我所提的问题属于哪个分类的学科。\\n问题：{question}'\n",
    ")\n",
    "functions = [\n",
    "    {\n",
    "        'name': 'question_category',\n",
    "        'description': '记录学科的分类',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'category': {\n",
    "                    'type': 'string',\n",
    "                    'description': '问题所对应的学科分类'\n",
    "                },\n",
    "                'question': {\n",
    "                    'type': 'string',\n",
    "                    'description': '我提问的问题'\n",
    "                }\n",
    "            },\n",
    "            'required': ['category', 'question']\n",
    "        }\n",
    "    }\n",
    "]\n",
    "chain = prompt | model.bind(\n",
    "    function_call={'name': 'question_category'},\n",
    "    functions=functions\n",
    ") | JsonOutputFunctionsParser()\n",
    "\n",
    "print(chain.invoke({'question': '牛顿第一定律是什么？'}))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 配置"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import ConfigurableField\n",
    "\n",
    "model = ChatOpenAI(temperature=0).configurable_fields(\n",
    "    temperature=ConfigurableField(\n",
    "        id='llm_temperature',\n",
    "        name='LLM Temperature',\n",
    "        description='LLM的温度设置',\n",
    "    )\n",
    ")\n",
    "prompt = PromptTemplate.from_template(\n",
    "    '请给我讲一个关于{type}的笑话'\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "res = chain.with_config(\n",
    "    configurable={'llm_temperature': 0.9}\n",
    ").invoke({'type': '程序员'})\n",
    "\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import ConfigurableField\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "story = PromptTemplate.from_template('请给我讲一个关于{type}的故事')\n",
    "joke = PromptTemplate.from_template('请给我讲一个关于{type}的冷笑话')\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    '请给我写一个关于{type}的演讲词'\n",
    ").configurable_alternatives(\n",
    "    ConfigurableField(id='prompt'),\n",
    "    story=story,\n",
    "    joke=joke,\n",
    "    # 设置默认模板对应的key\n",
    "    default_key='speech'\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "res = chain.with_config(\n",
    "    configurable={'prompt': 'joke'}\n",
    ").invoke({'type': '程序员'})\n",
    "\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 设置备用方案"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from openai import RateLimitError\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "openai_llm = ChatOpenAI(max_retries=0)\n",
    "anthropic_llm = ChatAnthropic()\n",
    "llm = openai_llm.with_fallbacks(\n",
    "    [anthropic_llm],\n",
    "    exceptions_to_handle=(RateLimitError,)\n",
    ")\n",
    "llm.invoke('请给我讲一个笑话')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 获取输入并运行自定义函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('question:{question}')\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "chain = (\n",
    "    {'question': RunnablePassthrough()} | prompt | llm\n",
    ")\n",
    "print(chain.invoke('你是谁？'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from operator import itemgetter\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('question:{question}')\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "chain = (\n",
    "    {'question': itemgetter('question')} | prompt | llm\n",
    ")\n",
    "print(chain.invoke({'question': '你是谁？', 'metadata': 'xxx'}))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('question:{question}')\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "def print_intput(text):\n",
    "    print(text)\n",
    "    return {'question': text}\n",
    "\n",
    "\n",
    "chain = (RunnableLambda(print_intput) | prompt | llm)\n",
    "print(chain.invoke('你是谁？'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('question:{question}')\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "\n",
    "def print_intput(text, config):\n",
    "    print(text)\n",
    "    print(config['foo'])\n",
    "    return {'question': text}\n",
    "\n",
    "\n",
    "chain = (RunnableLambda(print_intput) | prompt | llm)\n",
    "print(chain.invoke('你是谁？', config={'foo': 'bar'}))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('question:{question}')\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "def handle_metadata(data):\n",
    "    return {'user': 'xiaoming'}\n",
    "\n",
    "def handle_question(data):\n",
    "    return '请问，' + data['question']\n",
    "\n",
    "chain = (\n",
    "        RunnablePassthrough.assign(\n",
    "            metadata=handle_metadata,\n",
    "            question=handle_question,\n",
    "        )| RunnablePassthrough() | prompt | llm\n",
    ")\n",
    "\n",
    "print(chain.invoke({'question': '你是谁'}))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 路由链"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "model = ChatOpenAI()\n",
    "joke_tpl = ChatPromptTemplate.from_template('讲一个关于{type}的笑话')\n",
    "joke_chain = joke_tpl | model\n",
    "story_tpl = ChatPromptTemplate.from_template('讲一个关于{type}的故事')\n",
    "story_chain = story_tpl | model\n",
    "\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=story_chain)\n",
    "print(map_chain.invoke({'type': '动物'}))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.chains import create_tagging_chain_pydantic\n",
    "from langchain.schema.runnable import (\n",
    "    RouterRunnable, RunnablePassthrough\n",
    ")\n",
    "\n",
    "\n",
    "class SelectChain(BaseModel):\n",
    "    name: str = Field(description='从 \"physics\" 和 \"math\" 二选一')\n",
    "\n",
    "# 创建一个用于确定使用哪个选择链\n",
    "tagger = create_tagging_chain_pydantic(\n",
    "    SelectChain, ChatOpenAI(temperature=0)\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "physics_prompt_tpl = PromptTemplate.from_template(\n",
    "    '你是个优秀的物理学家，你很擅长用简明易懂的方式回答有关物理学的问题。'\n",
    "    '请使用英文帮我解答下列问题：\\n{question}'\n",
    ")\n",
    "math_prompt_tpl = PromptTemplate.from_template(\n",
    "    '你是个很好的数学家。你很擅长回答数学问题。'\n",
    "    '请使用英文帮我解答下列问题：\\n{question}'\n",
    ")\n",
    "\n",
    "# 创建一个路由链\n",
    "router = RouterRunnable({\n",
    "    'physics': physics_prompt_tpl | model,\n",
    "    'math': math_prompt_tpl | model\n",
    "})\n",
    "\n",
    "chain = {\n",
    "            'key': RunnablePassthrough() | tagger | (\n",
    "                lambda x: x['text'].name),\n",
    "            'input': {'question': RunnablePassthrough()}\n",
    "        } | router\n",
    "\n",
    "print(chain.invoke('物理中的牛顿第一定律是怎么定义的？'))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

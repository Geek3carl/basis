{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 回调"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 介绍"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "\n",
    "llm = OpenAI()\n",
    "prompt = PromptTemplate.from_template('1 + {number} = ')\n",
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "# 通过verbose=True的方式，隐式地使用StdOutCallbackHandler\n",
    "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "chain.run(number=2)\n",
    "\n",
    "# 在初始化Chain时定义回调\n",
    "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\n",
    "chain.run(number=2)\n",
    "\n",
    "# 在执行Chain时定义回调\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run(number=2, callbacks=[handler])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 自定义回调处理"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "\n",
    "class MyCustomHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f'My custom handler, token: {token}')\n",
    "\n",
    "\n",
    "# 通过streaming=True来启用流式输出，\n",
    "# on_llm_new_token方法只对流式输出起作用\n",
    "chat = ChatOpenAI(\n",
    "    max_tokens=25,\n",
    "    streaming=True,\n",
    "    callbacks=[MyCustomHandler()]\n",
    ")\n",
    "\n",
    "res = chat([HumanMessage(content='Tell me a joke')])\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 将日志记录到文件中"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import FileCallbackHandler\n",
    "\n",
    "# 全局debug开关，可以打印更详尽的输出内容\n",
    "# 同时，在langchain.globals中也提供了verbose全局开关set_verbose\n",
    "set_debug(True)\n",
    "\n",
    "logfile = 'output.log'\n",
    "\n",
    "logger.add(logfile, colorize=True, enqueue=True)\n",
    "handler = FileCallbackHandler(logfile)\n",
    "\n",
    "llm = OpenAI()\n",
    "prompt = PromptTemplate.from_template('1 + {number} = ')\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    callbacks=[handler],\n",
    "    verbose=True\n",
    ")\n",
    "answer = chain.run(number=2)\n",
    "logger.info(answer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ansi2html import Ansi2HTMLConverter\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "with open('output.log', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "conv = Ansi2HTMLConverter()\n",
    "html = conv.convert(content, full=True)\n",
    "\n",
    "display(HTML(html))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Token使用量跟踪"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.agents import (\n",
    "    AgentType, initialize_agent, load_tools\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-4-1106-preview',\n",
    "    temperature=0\n",
    ")\n",
    "tools = load_tools(['llm-math'], llm=llm)\n",
    "agent = initialize_agent(\n",
    "    tools, llm,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    response = agent.run('5的6次方是多少？')\n",
    "\n",
    "    print(f'Total Tokens: {cb.total_tokens}')\n",
    "    print(f'Prompt Tokens: {cb.prompt_tokens}')\n",
    "    print(f'Completion Tokens: {cb.completion_tokens}')\n",
    "    print(f'Total Cost (USD): ${cb.total_cost}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 隐私与安全"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 隐私"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_experimental.data_anonymizer import (\n",
    "    PresidioReversibleAnonymizer)\n",
    "\n",
    "# 初始化可逆匿名器\n",
    "anonymizer = PresidioReversibleAnonymizer(\n",
    "    # 需要匿名的字段\n",
    "    analyzed_fields=[\n",
    "        'PERSON', 'PHONE_NUMBER',\n",
    "        'EMAIL_ADDRESS', 'CREDIT_CARD'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 对内容进行匿名处理\n",
    "anonymized_text = anonymizer.anonymize(\n",
    "    'My name is Slim Shady, call me at 313-666-7440 or '\n",
    "    'email me at real.slim.shady@gmail.com. '\n",
    "    'By the way, my card number is: 4916 0387 9536 0861'\n",
    ")\n",
    "print(f'anonymized:\\n{anonymized_text}\\n')\n",
    "\n",
    "# 通过anonymizer.deanonymizer_mapping获取匿名前后字段的信息字典\n",
    "for code, info in anonymizer.deanonymizer_mapping.items():\n",
    "    print(f'code: {code} data:{info}')\n",
    "\n",
    "# 去匿名化\n",
    "text = anonymizer.deanonymize(anonymized_text)\n",
    "print(f'\\ndeanonymized:\\n{text}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_experimental.data_anonymizer import (\n",
    "    PresidioReversibleAnonymizer)\n",
    "\n",
    "anonymizer = PresidioReversibleAnonymizer(\n",
    "    analyzed_fields=['EMAIL_ADDRESS']\n",
    ")\n",
    "\n",
    "text = 'My email is \"foo@gmail.com\". I live in Hangzhou.'\n",
    "template = \"\"\"Please answer the question based on the information:\n",
    "\n",
    "{anonymized_text}\n",
    "\n",
    "question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template,\n",
    "    partial_variables={\n",
    "        # 重要信息匿名化\n",
    "        'anonymized_text': anonymizer.anonymize(text)\n",
    "    }\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "chain = (\n",
    "        {'question': RunnablePassthrough()} |\n",
    "        prompt | llm |\n",
    "        # 将LLM返回的信息去匿名化\n",
    "        (lambda ai_message: anonymizer.deanonymize(\n",
    "            ai_message.content\n",
    "        ))\n",
    ")\n",
    "response = chain.invoke('What is my email?')\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_experimental.data_anonymizer import (\n",
    "    PresidioReversibleAnonymizer)\n",
    "\n",
    "anonymizer = PresidioReversibleAnonymizer()\n",
    "anonymized_text = anonymizer.anonymize(\n",
    "    'My name is Slim Shady, call me at 313-666-7440 or '\n",
    "    'email me at real.slim.shady@gmail.com. '\n",
    "    'By the way, my card number is: 4916 0387 9536 0861'\n",
    ")\n",
    "\n",
    "# 将匿名器保存成本地文件\n",
    "# 支持JSON和YAML格式\n",
    "anonymizer.save_deanonymizer_mapping('deanonymizer_mapping.json')\n",
    "\n",
    "# 从本地文件中加载匿名器\n",
    "anonymizer_loader = PresidioReversibleAnonymizer()\n",
    "anonymizer.load_deanonymizer_mapping('deanonymizer_mapping.json')\n",
    "anonymizer.deanonymize(anonymized_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "from langchain_experimental.data_anonymizer import (\n",
    "    PresidioReversibleAnonymizer)\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "new_operators = {\n",
    "    'PERSON': OperatorConfig(\n",
    "        # 将fake名字反转\n",
    "        'custom', {'lambda': lambda _: fake.first_name_female()[::-1]}\n",
    "    ),\n",
    "}\n",
    "anonymizer = PresidioReversibleAnonymizer()\n",
    "anonymizer.add_operators(new_operators)\n",
    "anonymizer.anonymize('My name is LiaoKong')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 安全"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chains import OpenAIModerationChain\n",
    "\n",
    "# 当error值为False时，如果文本中有不当内容，不会抛出异常\n",
    "# 但会返回字符串\"Text was found that violates OpenAI's content policy.\"\n",
    "moderation_chain = OpenAIModerationChain(error=True)\n",
    "\n",
    "res = moderation_chain.run('This is okay')\n",
    "print(res + '\\n')\n",
    "res = moderation_chain.run('I will hit you')\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import (\n",
    "    OpenAIModerationChain, SequentialChain, LLMChain\n",
    ")\n",
    "\n",
    "# 实例化LLM Chain\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template='{setup}{new_input}Person2:'\n",
    ")\n",
    "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\n",
    "\n",
    "# 实例化审核链\n",
    "moderation_chain = OpenAIModerationChain()\n",
    "# 由于LLM Chain输出的key是text\n",
    "# 因此这里需要将下一个Chain（审核链）的输入设置为text\n",
    "moderation_chain.input_key = 'text'\n",
    "\n",
    "# 使用SequentialChain将两个Chain串起来\n",
    "chain = SequentialChain(\n",
    "    chains=[llm_chain, moderation_chain],\n",
    "    input_variables=['setup', 'new_input']\n",
    ")\n",
    "\n",
    "setup = '''We are playing a game of repeat after me.\n",
    "\n",
    "Person 1: Hi\n",
    "Person 2: Hi\n",
    "\n",
    "Person 1: How's your day\n",
    "Person 2: How's your day\n",
    "\n",
    "Person 1:'''\n",
    "new_input = 'I will hit you'\n",
    "inputs = {'setup': setup, 'new_input': new_input}\n",
    "\n",
    "res = chain(inputs, return_only_outputs=True)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation（评估）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 字符串评估器"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import Criteria, EvaluatorType\n",
    "\n",
    "# 初始化一个用于评估的LLM对象\n",
    "llm = ChatOpenAI(model='gpt-4-1106-preview', temperature=0)\n",
    "\n",
    "# 加载标准评估器\n",
    "evaluator = load_evaluator(\n",
    "    EvaluatorType.CRITERIA,\n",
    "    # 如果不设置，会使用GPT-4模型\n",
    "    llm=llm,\n",
    "    # 评估简单性\n",
    "    criteria=Criteria.CONCISENESS\n",
    ")\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    # 给定要评估的内容（output）\n",
    "    prediction=(\n",
    "        \"What's 2+2? That's an elementary question. \"\n",
    "        \"The answer you're looking for is that two and two is four.\"\n",
    "    ),\n",
    "    # 评估内容对应的问题\n",
    "    input=\"What's 2+2?\",\n",
    ")\n",
    "for key,value in eval_result.items():\n",
    "    print(f'{key}: {value}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import Criteria, EvaluatorType\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-1106-preview', temperature=0)\n",
    "\n",
    "evaluator = load_evaluator(\n",
    "    EvaluatorType.LABELED_CRITERIA,\n",
    "    llm=llm,\n",
    "    # 评估正确性\n",
    "    criteria=Criteria.CORRECTNESS\n",
    ")\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    input='中国的首都是哪里？',\n",
    "    prediction='中国的首都是北京',\n",
    "    # 设置参考答案\n",
    "    reference=(\n",
    "        '1949年9月27日，中国人民政治协商会议第一届全体会议'\n",
    "        '一致通过中华人民共和国的国都定于北平，即日起北平改名为北京。'\n",
    "    ),\n",
    ")\n",
    "for key, value in eval_result.items():\n",
    "    print(f'{key}: {value}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import EvaluatorType\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-1106-preview', temperature=0)\n",
    "\n",
    "custom_criterion = {\n",
    "    'numeric': '输出是否包含数字或数学信息？'\n",
    "}\n",
    "\n",
    "evaluator = load_evaluator(\n",
    "    EvaluatorType.CRITERIA,\n",
    "    criteria=custom_criterion,\n",
    ")\n",
    "\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction='正方形有4条相等的边',\n",
    "    input='正方形有几条边？'\n",
    ")\n",
    "for key, value in eval_result.items():\n",
    "    print(f'{key}: {value}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.evaluation import EmbeddingDistance\n",
    "\n",
    "evaluator = load_evaluator(\n",
    "    EvaluatorType.EMBEDDING_DISTANCE,\n",
    "    # 相似性计算算法，默认为EmbeddingDistance.COSINE\n",
    "    distance_metric=EmbeddingDistance.EUCLIDEAN,\n",
    ")\n",
    "\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction='I shall go',\n",
    "    reference='I will go'\n",
    ")\n",
    "print(eval_result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "from evaluate import load\n",
    "from langchain.evaluation import StringEvaluator\n",
    "\n",
    "class PerplexityEvaluator(StringEvaluator):\n",
    "    def __init__(self, model_id: str = 'gpt2'):\n",
    "        self.model_id = model_id\n",
    "        self.metric_fn = load(\n",
    "            'perplexity',\n",
    "            module_type='metric',\n",
    "            model_id=self.model_id,\n",
    "            pad_token=0\n",
    "        )\n",
    "\n",
    "    def _evaluate_strings(\n",
    "            self, *,\n",
    "            prediction: str,\n",
    "            reference: Optional[str] = None,\n",
    "            input: Optional[str] = None,\n",
    "            **kwargs: Any,\n",
    "    ) -> dict:\n",
    "        results = self.metric_fn.compute(\n",
    "            predictions=[prediction],\n",
    "            model_id=self.model_id\n",
    "        )\n",
    "        ppl = results['perplexities'][0]\n",
    "        return {'score': ppl}\n",
    "\n",
    "evaluator = PerplexityEvaluator()\n",
    "evaluator.evaluate_strings(\n",
    "    prediction='西班牙的雨水主要落在平原上'\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 比较评估器"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator, EvaluatorType\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-1106-preview', temperature=0)\n",
    "evaluator = load_evaluator(\n",
    "    EvaluatorType.LABELED_PAIRWISE_STRING,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate_string_pairs(\n",
    "    # 给定要评估的内容A\n",
    "    prediction='这有3只狗',\n",
    "    # 给定要评估的内容B\n",
    "    prediction_b='4',\n",
    "    # 用于评估的问题\n",
    "    input='公园里有几只狗？',\n",
    "    # 设置参考答案\n",
    "    reference=\"四只\",\n",
    ")\n",
    "\n",
    "for key, value in result.items():\n",
    "    print(f'{key}: {value}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import Criteria, EvaluatorType\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-1106-preview', temperature=0)\n",
    "\n",
    "evaluator = load_evaluator(\n",
    "    EvaluatorType.PAIRWISE_STRING,\n",
    "    llm=llm,\n",
    "    # 评估简单性\n",
    "    criteria=Criteria.CONCISENESS\n",
    ")\n",
    "eval_result = evaluator.evaluate_string_pairs(\n",
    "    prediction=\"The answer you're looking for is that two and two is four.\",\n",
    "    prediction_b=\"two plus two equals four\",\n",
    "    input=\"What's 2+2?\",\n",
    ")\n",
    "for key,value in eval_result.items():\n",
    "    print(f'{key}: {value}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.evaluation import load_evaluator, EvaluatorType\n",
    "\n",
    "\n",
    "custom_criteria = {\n",
    "    'simplicity': 'Is the language straightforward and unpretentious?',\n",
    "    'clarity': 'Are the sentences clear and easy to understand?',\n",
    "    'truthfulness': 'Does the writing feel honest and sincere?',\n",
    "    'subtext': 'Does the writing suggest deeper meanings or themes?',\n",
    "}\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-1106-preview', temperature=0)\n",
    "evaluator = load_evaluator(\n",
    "    EvaluatorType.PAIRWISE_STRING,\n",
    "    criteria=custom_criteria,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate_string_pairs(\n",
    "    prediction=(\n",
    "        'Every cheerful household shares a similar rhythm of joy; '\n",
    "        'but sorrow, in each household, plays a unique, '\n",
    "        'haunting melody.'\n",
    "    ),\n",
    "    prediction_b=(\n",
    "        'Where one finds a symphony of joy, '\n",
    "        'every domicile of happiness resounds in harmonious,'\n",
    "        ' identical notes; yet, every abode of despair conducts a '\n",
    "        'dissonant orchestra, each playing an elegy of grief '\n",
    "        'that is peculiar and profound to its own existence.'\n",
    "    ),\n",
    "    input='Write some prose about families.',\n",
    ")\n",
    "\n",
    "for key,value in result.items():\n",
    "    print(f'{key}: {value}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 轨迹评估器"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.evaluation import load_evaluator, EvaluatorType\n",
    "\n",
    "# 定义tools\n",
    "llm = ChatOpenAI(model='gpt-4-1106-preview', temperature=0)\n",
    "tools = load_tools(['ddg-search', 'llm-math'], llm=llm)\n",
    "\n",
    "# 初始化agent\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    # 需要返回中间步骤的信息，这很关键，评估器需要用到中间步骤数据\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "result = agent('OpenAI开发者大会召开的年份除以4，最后得到的数字是多少？')\n",
    "\n",
    "print('-------------result---------------')\n",
    "for key, value in result.items():\n",
    "    print(f'{key}: {value}')\n",
    "print('-------------evaluate---------------')\n",
    "\n",
    "# 评估agent\n",
    "evaluator = load_evaluator(\n",
    "    EvaluatorType.AGENT_TRAJECTORY,\n",
    "    llm=llm,\n",
    "    # 将用到的tools列表传给评估器，以便为评估器提供更多的上下文\n",
    "    agent_tools=tools\n",
    ")\n",
    "eval_result = evaluator.evaluate_agent_trajectory(\n",
    "    prediction=result['output'],\n",
    "    input=result['input'],\n",
    "    # 传入中间步骤\n",
    "    agent_trajectory=result['intermediate_steps'],\n",
    ")\n",
    "\n",
    "for key, value in eval_result.items():\n",
    "    print(f'{key}: {value}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Any, Optional, Sequence, Tuple\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import AgentAction\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.evaluation import AgentTrajectoryEvaluator\n",
    "\n",
    "\n",
    "class StepNecessityEvaluator(AgentTrajectoryEvaluator):\n",
    "    def __init__(self) -> None:\n",
    "        llm = ChatOpenAI(model='gpt-4-1106-preview', temperature=0.0)\n",
    "        template = '''Are any of the following steps unnecessary\n",
    "        in answering {input}? Provide the verdict on a new line\n",
    "        as a single \"Y\" for yes or \"N\" for no.\n",
    "\n",
    "        DATA\n",
    "        ------\n",
    "        Steps: {trajectory}\n",
    "        ------\n",
    "\n",
    "        Verdict:'''\n",
    "        self.chain = LLMChain.from_string(llm, template)\n",
    "\n",
    "    def _evaluate_agent_trajectory(\n",
    "            self, *,\n",
    "            prediction: str,\n",
    "            input: str,\n",
    "            agent_trajectory: Sequence[Tuple[AgentAction, str]],\n",
    "            reference: Optional[str] = None,\n",
    "            **kwargs: Any,\n",
    "    ) -> dict:\n",
    "        vals = [\n",
    "            (f'{i}: Action=[{action.tool}] '\n",
    "             f'returned observation = [{observation}]')\n",
    "            for i, (action, observation) in enumerate(agent_trajectory)\n",
    "        ]\n",
    "        trajectory = '\\n'.join(vals)\n",
    "        response = self.chain.run(\n",
    "            dict(trajectory=trajectory, input=input),\n",
    "            **kwargs\n",
    "        )\n",
    "        decision = response.split('\\n')[-1].strip()\n",
    "        # 如果操作是不必要的，则返回1\n",
    "        # 如果操作都是必要的，则返回0\n",
    "        score = 1 if decision == 'Y' else 0\n",
    "        return {'score': score, 'value': decision, 'reasoning': response}\n",
    "\n",
    "\n",
    "evaluator = StepNecessityEvaluator()\n",
    "\n",
    "res = evaluator.evaluate_agent_trajectory(\n",
    "    prediction='BC 221',\n",
    "    input='今天几号了？',\n",
    "    agent_trajectory=[\n",
    "        (\n",
    "            AgentAction(tool='ask', tool_input='今天几号了？', log=''),\n",
    "            '明天就是昨天',\n",
    "        ),\n",
    "        (\n",
    "            AgentAction(tool='foo', tool_input='看电视半小时', log=''),\n",
    "            'foo',\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "for key, value in res.items():\n",
    "    print(f'{key}: {value}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LangSmith"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 收集与追踪"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "# 可以在Settings页面获取\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'cl__xxxx'\n",
    "# 可以先在Projects页面进行创建，如果这里设置的没有创建，会自动创建这个项目\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'langchain_learn'\n",
    "\n",
    "# 因为LangSmith是非侵入式的，所以不需要添加任何的LangSmith相关代码\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "# 定义tools\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "tools = load_tools(['ddg-search', 'llm-math'], llm=llm)\n",
    "\n",
    "# 初始化agent\n",
    "agent = initialize_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION\n",
    ")\n",
    "\n",
    "agent('OpenAI开发者大会召开的年份除以4，最后得到的数字是多少？')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "example_inputs = [\n",
    "  (\"What is the largest mammal?\", \"The blue whale\"),\n",
    "  (\"What do mammals and birds have in common?\",\n",
    "   \"They are both warm-blooded\"),\n",
    "  (\"What are reptiles known for?\", \"Having scales\"),\n",
    "  (\"What's the main characteristic of amphibians?\",\n",
    "   \"They live both in water and on land\"),\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Elementary Animal Questions\"\n",
    "\n",
    "# 创建数据集\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Questions and answers about animal phylogenetics\",\n",
    ")\n",
    "\n",
    "# 将数据批量添加到数据集中\n",
    "for input_prompt, output_answer in example_inputs:\n",
    "    client.create_example(\n",
    "        inputs={\"question\": input_prompt},\n",
    "        outputs={\"answer\": output_answer},\n",
    "        dataset_id=dataset.id,\n",
    "    )\n",
    "\n",
    "# 获取数据集的内容\n",
    "examples = client.list_examples(dataset_name=dataset_name)\n",
    "print(examples)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 评估"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "example_inputs = [\n",
    "  (\"What is the largest mammal?\", \"The blue whale\"),\n",
    "  (\"What do mammals and birds have in common?\",\n",
    "   \"They are both warm-blooded\"),\n",
    "  (\"What are reptiles known for?\", \"Having scales\"),\n",
    "  (\"What's the main characteristic of amphibians?\",\n",
    "   \"They live both in water and on land\"),\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Elementary Animal Questions\"\n",
    "\n",
    "# 创建数据集\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Questions and answers about animal phylogenetics\",\n",
    ")\n",
    "\n",
    "# 将数据批量添加到数据集中\n",
    "for input_prompt, output_answer in example_inputs:\n",
    "    client.create_example(\n",
    "        inputs={\"question\": input_prompt},\n",
    "        outputs={\"answer\": output_answer},\n",
    "        dataset_id=dataset.id,\n",
    "    )\n",
    "\n",
    "# 获取数据集的内容\n",
    "examples = client.list_examples(dataset_name=dataset_name)\n",
    "print(examples)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hub"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# 从Hub中拉取Prompt模板\n",
    "prompt = hub.pull('liaokong/my-first-prompt')\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# 创建一个Chain并运行\n",
    "runnable = prompt | model\n",
    "runnable.invoke({\n",
    "\t'profession': 'biologist',\n",
    "    'question': 'What is special about parrots?',\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    'tell me a joke about {topic}'\n",
    ")\n",
    "\n",
    "# 需要将路径前缀改成你的用户名\n",
    "hub.push('liaokong/topic-joke-generator', prompt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LangServe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 构建"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "\n",
    "# language和content会自动转换成接口的两个请求参数\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    'You are a capable assistant in translating the '\n",
    "    'following content into {language}.\\n{content}'\n",
    ")\n",
    "\n",
    "app = FastAPI(title='LangChain Server')\n",
    "\n",
    "# 添加一个用于翻译的接口\n",
    "add_routes(app, prompt | ChatOpenAI(), path='/translate')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host='0.0.0.0', port=8000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 调用"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.post(\n",
    "    'http://127.0.0.1:8000/translate/invoke',\n",
    "    json={\n",
    "        'input': {\n",
    "            'content': '你是谁？',\n",
    "            'language': 'english'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(response.json())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "chain = RemoteRunnable('http://127.0.0.1:8000/translate/')\n",
    "print(chain.invoke(\n",
    "    {'content': '你是谁？', 'language': 'english'}\n",
    "))\n",
    "\n",
    "# 也支持流式输出\n",
    "import asyncio\n",
    "\n",
    "async def run():\n",
    "    async for msg in chain.astream(\n",
    "            {'content': '你是谁？', 'language': 'english'}\n",
    "    ):\n",
    "        print(msg, end='', flush=True)\n",
    "\n",
    "\n",
    "asyncio.run(run())"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

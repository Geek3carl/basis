{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 安装 LangChain"
   ],
   "metadata": {
    "id": "rfO7MCKbgiDr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtQa-iYagZ90",
    "outputId": "2f1589d7-6f7e-4aa5-ac6a-45d6b5b589a8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting langchain[llms]\n",
      "  Downloading langchain-0.0.314-py3-none-any.whl (1.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m21.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (2.0.21)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (4.0.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain[llms])\n",
      "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain[llms])\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.43 (from langchain[llms])\n",
      "  Downloading langsmith-0.0.43-py3-none-any.whl (40 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.0/40.0 kB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (1.23.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (1.10.13)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (8.2.3)\n",
      "Collecting clarifai>=9.1.0 (from langchain[llms])\n",
      "  Downloading clarifai-9.9.2-py3-none-any.whl (2.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m58.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting cohere<5,>=4 (from langchain[llms])\n",
      "  Downloading cohere-4.27-py3-none-any.whl (47 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m47.6/47.6 kB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting huggingface_hub<1,>=0 (from langchain[llms])\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m302.0/302.0 kB\u001B[0m \u001B[31m24.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting manifest-ml<0.0.2,>=0.0.1 (from langchain[llms])\n",
      "  Downloading manifest_ml-0.0.1-py2.py3-none-any.whl (42 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m42.9/42.9 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nlpcloud<2,>=1 (from langchain[llms])\n",
      "  Downloading nlpcloud-1.1.44-py3-none-any.whl (4.4 kB)\n",
      "Collecting openai<1,>=0 (from langchain[llms])\n",
      "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m77.0/77.0 kB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting openlm<0.0.6,>=0.0.5 (from langchain[llms])\n",
      "  Downloading openlm-0.0.5-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: torch<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (2.0.1+cu118)\n",
      "Collecting transformers<5,>=4 (from langchain[llms])\n",
      "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.7/7.7 MB\u001B[0m \u001B[31m53.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain[llms]) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain[llms]) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain[llms]) (1.1.3)\n",
      "Collecting clarifai-grpc>=9.8.1 (from clarifai>=9.1.0->langchain[llms])\n",
      "  Downloading clarifai_grpc-9.9.0-py3-none-any.whl (218 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m218.7/218.7 kB\u001B[0m \u001B[31m21.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tritonclient==2.34.0 (from clarifai>=9.1.0->langchain[llms])\n",
      "  Downloading tritonclient-2.34.0-py3-none-manylinux1_x86_64.whl (12.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.3/12.3 MB\u001B[0m \u001B[31m71.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clarifai>=9.1.0->langchain[llms]) (23.2)\n",
      "Collecting tqdm==4.64.1 (from clarifai>=9.1.0->langchain[llms])\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.5/78.5 kB\u001B[0m \u001B[31m8.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting rich==13.4.2 (from clarifai>=9.1.0->langchain[llms])\n",
      "  Downloading rich-13.4.2-py3-none-any.whl (239 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m239.4/239.4 kB\u001B[0m \u001B[31m17.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting schema==0.7.5 (from clarifai>=9.1.0->langchain[llms])\n",
      "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich==13.4.2->clarifai>=9.1.0->langchain[llms]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich==13.4.2->clarifai>=9.1.0->langchain[llms]) (2.16.1)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema==0.7.5->clarifai>=9.1.0->langchain[llms]) (21.6.0)\n",
      "Collecting python-rapidjson>=0.9.1 (from tritonclient==2.34.0->clarifai>=9.1.0->langchain[llms])\n",
      "  Downloading python_rapidjson-1.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m51.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting backoff<3.0,>=2.0 (from cohere<5,>=4->langchain[llms])\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting fastavro==1.8.2 (from cohere<5,>=4->langchain[llms])\n",
      "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.7/2.7 MB\u001B[0m \u001B[31m65.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere<5,>=4->langchain[llms]) (6.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere<5,>=4->langchain[llms]) (2.0.6)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain[llms])\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.4/49.4 kB\u001B[0m \u001B[31m4.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain[llms])\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1,>=0->langchain[llms]) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1,>=0->langchain[llms]) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1,>=0->langchain[llms]) (4.5.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain[llms])\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting dill>=0.3.5 (from manifest-ml<0.0.2,>=0.0.1->langchain[llms])\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m115.3/115.3 kB\u001B[0m \u001B[31m7.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting redis>=4.3.1 (from manifest-ml<0.0.2,>=0.0.1->langchain[llms])\n",
      "  Downloading redis-5.0.1-py3-none-any.whl (250 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m250.3/250.3 kB\u001B[0m \u001B[31m16.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting sqlitedict>=2.0.0 (from manifest-ml<0.0.2,>=0.0.1->langchain[llms])\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain[llms]) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain[llms]) (3.0.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1->langchain[llms]) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1->langchain[llms]) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1->langchain[llms]) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1->langchain[llms]) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<3,>=1->langchain[llms]) (3.27.6)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<3,>=1->langchain[llms]) (17.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4->langchain[llms]) (2023.6.3)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers<5,>=4->langchain[llms])\n",
      "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.8/3.8 MB\u001B[0m \u001B[31m99.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting safetensors>=0.3.1 (from transformers<5,>=4->langchain[llms])\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m60.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: grpcio>=1.44.0 in /usr/local/lib/python3.10/dist-packages (from clarifai-grpc>=9.8.1->clarifai>=9.1.0->langchain[llms]) (1.59.0)\n",
      "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from clarifai-grpc>=9.8.1->clarifai>=9.1.0->langchain[llms]) (3.20.3)\n",
      "Requirement already satisfied: googleapis-common-protos>=1.53.0 in /usr/local/lib/python3.10/dist-packages (from clarifai-grpc>=9.8.1->clarifai>=9.1.0->langchain[llms]) (1.60.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere<5,>=4->langchain[llms]) (3.17.0)\n",
      "Collecting huggingface_hub<1,>=0 (from langchain[llms])\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m295.0/295.0 kB\u001B[0m \u001B[31m26.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain[llms])\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1->langchain[llms]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=1->langchain[llms]) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich==13.4.2->clarifai>=9.1.0->langchain[llms]) (0.1.2)\n",
      "Building wheels for collected packages: sqlitedict\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=875db238eb8b50f59aea550991b8adc457d94a81880ead12d995b9771c395b17\n",
      "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
      "Successfully built sqlitedict\n",
      "Installing collected packages: sqlitedict, tqdm, schema, safetensors, redis, python-rapidjson, mypy-extensions, marshmallow, jsonpointer, fastavro, dill, backoff, typing-inspect, tritonclient, rich, openlm, nlpcloud, manifest-ml, langsmith, jsonpatch, huggingface_hub, clarifai-grpc, tokenizers, openai, dataclasses-json, cohere, clarifai, transformers, langchain\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 13.6.0\n",
      "    Uninstalling rich-13.6.0:\n",
      "      Successfully uninstalled rich-13.6.0\n",
      "Successfully installed backoff-2.2.1 clarifai-9.9.2 clarifai-grpc-9.9.0 cohere-4.27 dataclasses-json-0.6.1 dill-0.3.7 fastavro-1.8.2 huggingface_hub-0.17.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.314 langsmith-0.0.43 manifest-ml-0.0.1 marshmallow-3.20.1 mypy-extensions-1.0.0 nlpcloud-1.1.44 openai-0.28.1 openlm-0.0.5 python-rapidjson-1.12 redis-5.0.1 rich-13.4.2 safetensors-0.4.0 schema-0.7.5 sqlitedict-2.1.0 tokenizers-0.14.1 tqdm-4.64.1 transformers-4.34.0 tritonclient-2.34.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain[llms]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 设置 OpenAI Key"
   ],
   "metadata": {
    "id": "FY5LeIhCgnfq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = ''"
   ],
   "metadata": {
    "id": "mLWoDchagrYS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Prompt Templates（提示词模板）"
   ],
   "metadata": {
    "id": "8fyHMZWkgvbj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PromptTemplate"
   ],
   "metadata": {
    "id": "9pJPQc9GE1Zt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = '请帮我生成{language}的打印\"hello world\"的代码示例'\n",
    "prompt_tpl = PromptTemplate.from_template(template)\n",
    "\n",
    "llm = OpenAI(temperature=0.1)\n",
    "for language in ['Python', 'Java', 'C++']:\n",
    "    prompt = prompt_tpl.format(language=language)\n",
    "    print(f'{\"=\"* 10} {language} {\"=\"* 10}')\n",
    "    print(llm(prompt))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIs3Za1Jg15C",
    "outputId": "d25125e7-58ed-4da0-e4b2-90bd320bee47"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "========== Python ==========\n",
      "\n",
      "\n",
      "print(\"Hello World\")\n",
      "========== Java ==========\n",
      "\n",
      "\n",
      "public class HelloWorld {\n",
      "    public static void main(String[] args) {\n",
      "        System.out.println(\"Hello World!\");\n",
      "    }\n",
      "}\n",
      "========== C++ ==========\n",
      "\n",
      "\n",
      "#include <iostream>\n",
      "\n",
      "int main()\n",
      "{\n",
      "    std::cout << \"Hello World!\" << std::endl;\n",
      "    return 0;\n",
      "}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Partial Prompt Templates"
   ],
   "metadata": {
    "id": "SJ0IWTqhE53s"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_tpl = PromptTemplate.from_template(\n",
    "    '请给你我讲{num}个关于{type}的笑话,'\n",
    "    '并且不要出现{location1}和{location2}'\n",
    ")\n",
    "\n",
    "partial_prompt1 = prompt_tpl.partial(num='3')\n",
    "partial_prompt2 = partial_prompt1.partial(\n",
    "    location1='办公室',\n",
    "    location2='学校'\n",
    ")\n",
    "print(partial_prompt2.format(type='程序员'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6VA3krCrWer",
    "outputId": "197cdde1-8cdc-424f-b93a-4a87c9181a24"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "请给你我讲3个关于程序员的笑话,并且不要出现办公室和学校\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def get_date():\n",
    "    return datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "prompt_tpl = PromptTemplate.from_template(\n",
    "    '你是一个优秀的{role}助手，你的知识库截止日期是{date}'\n",
    ")\n",
    "\n",
    "partial_prompt = prompt_tpl.partial(date=get_date)\n",
    "print(partial_prompt.format(role='AI'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "InQ9k65zrWOb",
    "outputId": "bc41c653-9ad2-4c66-ee81-ddcbfc2687f2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "你是一个优秀的AI助手，你的知识库截止日期是2023-10-08\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def get_date():\n",
    "    return datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "prompt_tpl = PromptTemplate.from_template(\n",
    "    '你是一个优秀的{role}助手，你的知识库截止日期是{date}',\n",
    "    partial_variables={'date': get_date}\n",
    ")\n",
    "\n",
    "print(prompt_tpl.format(role='AI'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KLY-Ljmqt5bZ",
    "outputId": "7e13098b-fbe1-40ec-8d9b-a06cc0383336"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "你是一个优秀的AI助手，你的知识库截止日期是2023-10-08\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PipelinePromptTemplate"
   ],
   "metadata": {
    "id": "zyuzMUtCFQzl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "full_template = '''\n",
    "{expect}\n",
    "{example}\n",
    "{question}\n",
    "'''\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "\n",
    "expect_prompt = PromptTemplate.from_template(\n",
    "    '请学习我给定的例子，并判断我给出的提问：'\n",
    ")\n",
    "\n",
    "example_prompt = PromptTemplate.from_template('''\"\"\"\n",
    "文本：今天天气阳光明媚，真好！\n",
    "情绪：正向\n",
    "\n",
    "文本：今天又下雨了，天气真糟糕！\n",
    "情绪：反向\n",
    "\n",
    "文本：今天衣服又弄脏了！\n",
    "情绪：反向\n",
    "\"\"\"''')\n",
    "\n",
    "question_prompt = PromptTemplate.from_template('''\n",
    "文本：{input}！\n",
    "情绪：\n",
    "''')\n",
    "\n",
    "input_prompts = [\n",
    "    ('expect', expect_prompt),\n",
    "    ('example', example_prompt),\n",
    "    ('question', question_prompt)\n",
    "]\n",
    "\n",
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=full_prompt,\n",
    "    pipeline_prompts=input_prompts\n",
    ")\n",
    "\n",
    "print(pipeline_prompt.format(input='今天又被批评了'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrKMnoeJxrdt",
    "outputId": "fa10409f-cd39-4f33-96c3-9aef46d2f11e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "请学习我给定的例子，并判断我给出的提问：\n",
      "\"\"\"\n",
      "文本：今天天气阳光明媚，真好！\n",
      "情绪：正向\n",
      "\n",
      "文本：今天又下雨了，天气真糟糕！\n",
      "情绪：反向\n",
      "\n",
      "文本：今天衣服又弄脏了！\n",
      "情绪：反向\n",
      "\"\"\"\n",
      "\n",
      "文本：今天又被批评了！\n",
      "情绪：\n",
      "\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FewShotPromptTemplate"
   ],
   "metadata": {
    "id": "vOoYR4AuFWTz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    '文本：{text}\\n情绪：{mood}'\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {'text': '今天天气阳光明媚，真好！', 'mood': '正向'},\n",
    "    {'text': '今天又下雨了，天气真糟糕！', 'mood': '反向'},\n",
    "    {'text': '今天衣服又弄脏了！', 'mood': '反向'},\n",
    "]\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    prefix='请学习我给定的例子，并判断我给出的提问：\\n\"\"\"',\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    suffix='\"\"\"\\n文本：{input}！\\n情绪：',\n",
    "    input_variables=['input']\n",
    ")\n",
    "\n",
    "print(prompt.format(input='今天又被批评了！'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kTxqF2D_m3l",
    "outputId": "05b0060b-209c-4563-833f-3afca3852ab0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "请学习我给定的例子，并判断我给出的提问：\n",
      "\"\"\"\n",
      "\n",
      "文本：今天天气阳光明媚，真好！\n",
      "情绪：正向\n",
      "\n",
      "文本：今天又下雨了，天气真糟糕！\n",
      "情绪：反向\n",
      "\n",
      "文本：今天衣服又弄脏了！\n",
      "情绪：反向\n",
      "\n",
      "\"\"\"\n",
      "文本：今天又被批评了！！\n",
      "情绪：\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 自定义 Prompt Template"
   ],
   "metadata": {
    "id": "A5syBj3wFbwr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import types\n",
    "import random\n",
    "\n",
    "from langchain.utils.formatting import formatter\n",
    "from langchain.prompts.base import StringPromptTemplate\n",
    "\n",
    "\n",
    "class FunctionPromptTemplate(StringPromptTemplate):\n",
    "    template = ''\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        kwargs = self._merge_partial_and_user_variables(**kwargs)\n",
    "\n",
    "        for key, value in kwargs.items():\n",
    "            if isinstance(value, types.FunctionType):\n",
    "                kwargs[key] = value()\n",
    "\n",
    "        return formatter.format(self.template, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def _prompt_type(self) -> str:\n",
    "        return 'function_prompt'\n",
    "\n",
    "\n",
    "def get_num():\n",
    "    return random.randint(0, 10)\n",
    "\n",
    "\n",
    "prompt = FunctionPromptTemplate(\n",
    "    template='请给你我讲{num}个笑话',\n",
    "    input_variables=['num'],\n",
    ")\n",
    "\n",
    "\n",
    "print(prompt.format(num=get_num))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJnt9TrLDZFG",
    "outputId": "c6d9b3c8-d211-4d9f-d04e-0a49985ef317"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "请给你我讲7个笑话\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prompt Template 的序列化与反序列化"
   ],
   "metadata": {
    "id": "raFF_GdiFg87"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.loading import load_prompt\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='请给你我讲{num}个笑话',\n",
    "    input_variables=['num'],\n",
    ")\n",
    "prompt.save('prompt.json')\n",
    "\n",
    "prompt = load_prompt('prompt.json')\n",
    "print(prompt)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5rkRxcQDfWm",
    "outputId": "715fefea-0db9-418b-8690-3b0a525b6ee8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_variables=['num'] template='请给你我讲{num}个笑话'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ChatPromptTemplate"
   ],
   "metadata": {
    "id": "RFdOGBjwFj50"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatMessagePromptTemplate\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        '你的名字是{name}'),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        '你叫什么名字？')\n",
    "]\n",
    "\n",
    "# 或者使用 ChatMessagePromptTemplate.from_template 方法\n",
    "# 但是需要设置对应的 role 参数\n",
    "# messages = [\n",
    "#     ChatMessagePromptTemplate.from_template(\n",
    "#         '你的名字是{name}', role='system'),\n",
    "#     ChatMessagePromptTemplate.from_template(\n",
    "#         '你叫什么名字？', role='human')\n",
    "# ]\n",
    "\n",
    "prompt_tpl = ChatPromptTemplate(\n",
    "    messages=messages,\n",
    "    input_variables=['name']\n",
    ")\n",
    "\n",
    "print(prompt_tpl)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKmohv7cGFtD",
    "outputId": "519ef5ff-1137-4c05-d731-888cb9f7b2f7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_variables=['name'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], template='你的名字是{name}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='你叫什么名字？'))]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "messages = [\n",
    "    ('system', '你的名字是{name}'),\n",
    "    ('human', '你叫什么名字？')\n",
    "]\n",
    "\n",
    "prompt_tpl = ChatPromptTemplate.from_messages(\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "prompt = prompt_tpl.format_messages(name='小明')\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "print(llm(prompt))\n"
   ],
   "metadata": {
    "id": "xIlgYpPqvl_t",
    "outputId": "74773802-ec0d-457a-81c3-dbd34072b14c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "content='我是小明。'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MessagesPlaceholder"
   ],
   "metadata": {
    "id": "O9agkWwIFtZj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        '请用不超过{text_number}个字来总结以下对话'),\n",
    "    MessagesPlaceholder(variable_name='context'),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        '###请开始总结上面的对话')\n",
    "]\n",
    "\n",
    "prompt_tpl = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "human_message = HumanMessage(content='如何学好英语？')\n",
    "ai_message = AIMessage(\n",
    "    content='学好英语需要每天持续实践，均衡地练习听、说、读、写四大技能，'\n",
    "            '不断扩充词汇和掌握语法。利用现代技术工具可增强学习效果，'\n",
    "            '考虑沉浸式学习方法并参与相关课程与学习小组。逐渐增加阅读难度，'\n",
    "            '模仿优秀的英语说话者，定期反思并调整学习方法，'\n",
    "            '并始终保持积极的学习态度。'\n",
    ")\n",
    "\n",
    "prompt_messages = prompt_tpl.format_messages(\n",
    "    context=[human_message, ai_message], text_number=20)\n",
    "\n",
    "for message in prompt_messages:\n",
    "    print(repr(message))\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "print('\\n' + repr(llm(prompt_messages)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cw8YNGEIt-bg",
    "outputId": "3189aa92-446f-4644-dff1-8312a93ecf58"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SystemMessage(content='请用不超过20个字来总结以下对话')\n",
      "HumanMessage(content='如何学好英语？')\n",
      "AIMessage(content='学好英语需要每天持续实践，均衡地练习听、说、读、写四大技能，不断扩充词汇和掌握语法。利用现代技术工具可增强学习效果，考虑沉浸式学习方法并参与相关课程与学习小组。逐渐增加阅读难度，模仿优秀的英语说话者，定期反思并调整学习方法，并始终保持积极的学习态度。')\n",
      "HumanMessage(content='###请开始总结上面的对话')\n",
      "\n",
      "AIMessage(content='学好英语：坚持每天实践，全面练习，利用技术工具，沉浸式学习，模仿优秀者，积极态度。')\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example Selector（示例选择器）"
   ],
   "metadata": {
    "id": "bZBMDzaZwZd7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FewShotChatMessagePromptTemplate"
   ],
   "metadata": {
    "id": "EraR2ZmzFsFL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {'input': '2+2', 'output': '4'},\n",
    "    {'input': '2+3', 'output': '5'},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('human', '{input}'),\n",
    "        ('ai', '{output}'),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "print(few_shot_prompt.format())\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D-nMqPz_t4V",
    "outputId": "0dac1608-5722-4ad8-8696-7fbb64c28a53"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Human: 2+2\n",
      "AI: 4\n",
      "Human: 2+3\n",
      "AI: 5\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LengthBasedExampleSelector"
   ],
   "metadata": {
    "id": "vKqi7I2GTqFV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    'Input: {input}\\noutput: {output}'\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {'input': 'happy', 'output': 'sad'},\n",
    "    {'input': 'tall', 'output': 'short'},\n",
    "    {'input': 'hot', 'output': 'cold'},\n",
    "    {'input': 'fast', 'output': 'slow'},\n",
    "    {'input': 'rich', 'output': 'poor'},\n",
    "]\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    "    max_length=12,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    prefix='Please learn examples and answer questions:',\n",
    "    example_prompt=example_prompt,\n",
    "    suffix='Input: {input}\\nOutput:',\n",
    "    input_variables=['input']\n",
    ")\n",
    "\n",
    "print(prompt.format(input='open'))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frbSkBqRLO6K",
    "outputId": "d08516a5-3b09-46c3-d7bf-20c18ca6c58c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Please learn examples and answer questions:\n",
      "\n",
      "Input: happy\n",
      "output: sad\n",
      "\n",
      "Input: tall\n",
      "output: short\n",
      "\n",
      "Input: open\n",
      "Output:\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SemanticSimilarityExampleSelector"
   ],
   "metadata": {
    "id": "yHjQrl7TTkht"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install chromadb\n",
    "!pip install tiktoken"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJuAT2izVV2J",
    "outputId": "68205f38-cfb1-4678-ae13-bfc257b8057d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-0.4.14-py3-none-any.whl (448 kB)\n",
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/448.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m102.4/448.1 kB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m448.1/448.1 kB\u001B[0m \u001B[31m6.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.13)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.4/2.4 MB\u001B[0m \u001B[31m44.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
      "  Downloading fastapi-0.103.2-py3-none-any.whl (66 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m66.3/66.3 kB\u001B[0m \u001B[31m6.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
      "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m59.5/59.5 kB\u001B[0m \u001B[31m5.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
      "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.4/5.4 MB\u001B[0m \u001B[31m71.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.2/6.2 MB\u001B[0m \u001B[31m78.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.14.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.3/67.3 kB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting tqdm>=4.65.0 (from chromadb)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.3/78.3 kB\u001B[0m \u001B[31m7.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.59.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m593.7/593.7 kB\u001B[0m \u001B[31m40.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.0/67.0 kB\u001B[0m \u001B[31m6.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.0/46.0 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (2.0.6)\n",
      "Requirement already satisfied: huggingface_hub<0.18,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.17.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m58.3/58.3 kB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m428.8/428.8 kB\u001B[0m \u001B[31m32.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.1/4.1 MB\u001B[0m \u001B[31m79.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m67.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m129.9/129.9 kB\u001B[0m \u001B[31m12.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.1.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.12.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m86.8/86.8 kB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=73473c4fa83d6cb492ea632d312223c73a352c99573c33e1914b671ba799776d\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, websockets, uvloop, tqdm, python-dotenv, pulsar-client, overrides, humanfriendly, httptools, h11, chroma-hnswlib, bcrypt, watchfiles, uvicorn, starlette, posthog, coloredlogs, onnxruntime, fastapi, chromadb\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "clarifai 9.9.1 requires tqdm==4.64.1, but you have tqdm 4.66.1 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed bcrypt-4.0.1 chroma-hnswlib-0.7.3 chromadb-0.4.14 coloredlogs-15.0.1 fastapi-0.103.2 h11-0.14.0 httptools-0.6.0 humanfriendly-10.0 monotonic-1.6 onnxruntime-1.16.0 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 tqdm-4.66.1 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.20.0 websockets-11.0.3\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m8.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.5.1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import SemanticSimilarityExampleSelector\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {'input': 'happy', 'output': 'sad'},\n",
    "    {'input': 'tall', 'output': 'short'},\n",
    "    {'input': 'hot', 'output': 'cold'},\n",
    "    {'input': 'fast', 'output': 'slow'},\n",
    "    {'input': 'rich', 'output': 'poor'},\n",
    "]\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples=examples,\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    "    vectorstore_cls=Chroma,\n",
    "    k=1\n",
    ")\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    'Input: {input}\\noutput: {output}'\n",
    ")\n",
    "\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    prefix='Please learn examples and answer questions:',\n",
    "    example_prompt=example_prompt,\n",
    "    suffix='Input: {input}\\nOutput:',\n",
    "    input_variables=['input']\n",
    ")\n",
    "\n",
    "print(prompt.format(input='sunny'))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8G3xQL-nVLqI",
    "outputId": "5e05e186-2d1c-4d9d-d6e1-8a588c4bd60e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-YFoRhGzEkwdfbqtdHb3Z5hPM on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-YFoRhGzEkwdfbqtdHb3Z5hPM on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-YFoRhGzEkwdfbqtdHb3Z5hPM on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Please learn examples and answer questions:\n",
      "\n",
      "Input: happy\n",
      "output: sad\n",
      "\n",
      "Input: sunny\n",
      "Output:\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MaxMarginalRelevanceExampleSelector"
   ],
   "metadata": {
    "id": "SWUvRwJMTgrG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import MaxMarginalRelevanceExampleSelector\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {'input': 'happy', 'output': 'sad'},\n",
    "    {'input': 'tall', 'output': 'short'},\n",
    "    {'input': 'hot', 'output': 'cold'},\n",
    "    {'input': 'fast', 'output': 'slow'},\n",
    "    {'input': 'rich', 'output': 'poor'},\n",
    "]\n",
    "\n",
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    examples=examples,\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    "    vectorstore_cls=Chroma,\n",
    "    k=2\n",
    ")\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    'Input: {input}\\noutput: {output}'\n",
    ")\n",
    "\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    prefix='Please learn examples and answer questions:',\n",
    "    example_prompt=example_prompt,\n",
    "    suffix='Input: {input}\\nOutput:',\n",
    "    input_variables=['input']\n",
    ")\n",
    "\n",
    "print(prompt.format(input='sunny'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FG49XC0cY7j0",
    "outputId": "a5160e7f-17f6-4703-f0a9-842d01f43364"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 10, updating n_results = 10\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Please learn examples and answer questions:\n",
      "\n",
      "Input: happy\n",
      "output: sad\n",
      "\n",
      "Input: tall\n",
      "output: short\n",
      "\n",
      "Input: sunny\n",
      "Output:\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install nltk"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vUiRw9jrWh2r",
    "outputId": "b63b1b2d-e537-49f3-eb4d-f95c1027b119"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.64.1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.prompts import NGramOverlapExampleSelector\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {'input': 'happy', 'output': 'sad'},\n",
    "    {'input': 'tall', 'output': 'short'},\n",
    "    {'input': 'hot', 'output': 'cold'},\n",
    "    {'input': 'fast', 'output': 'slow'},\n",
    "    {'input': 'rich', 'output': 'poor'},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    'Input: {input}\\noutput: {output}'\n",
    ")\n",
    "\n",
    "example_selector = NGramOverlapExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    threshold=-1,\n",
    "\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    prefix='Please learn examples and answer questions:',\n",
    "    example_prompt=example_prompt,\n",
    "    suffix='Input: {input}\\nOutput:',\n",
    "    input_variables=['input']\n",
    ")\n",
    "\n",
    "print(prompt.format(input='sunny'))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SuXHyFBMWqyC",
    "outputId": "4c24fc9d-e7a3-43ec-95a8-7a7cb3c47075"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Please learn examples and answer questions:\n",
      "\n",
      "Input: happy\n",
      "output: sad\n",
      "\n",
      "Input: tall\n",
      "output: short\n",
      "\n",
      "Input: hot\n",
      "output: cold\n",
      "\n",
      "Input: fast\n",
      "output: slow\n",
      "\n",
      "Input: rich\n",
      "output: poor\n",
      "\n",
      "Input: sunny\n",
      "Output:\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.prompts import NGramOverlapExampleSelector\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {'input': 'See Spot run.', 'output': 'Ver correr a Spot.'},\n",
    "    {'input': 'My dog barks.', 'output': 'Mi perro ladra.'},\n",
    "    {'input': 'Spot can run.', 'output': 'Spot puede correr.'},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    'Input: {input}\\noutput: {output}'\n",
    ")\n",
    "\n",
    "example_selector = NGramOverlapExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    threshold=-1,\n",
    "\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    prefix='Please learn examples and answer questions:',\n",
    "    example_prompt=example_prompt,\n",
    "    suffix='Input: {input}\\nOutput:',\n",
    "    input_variables=['input']\n",
    ")\n",
    "\n",
    "\n",
    "for threshold in [-0.1, 0.01, 1.0]:\n",
    "    print(f'\\n======= threshold: {threshold} =======')\n",
    "    example_selector.threshold = threshold\n",
    "    print(prompt.format(input='Spot can run fast.'))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Joerorg5Yjyg",
    "outputId": "aac0ef55-268d-43e8-b40c-f91a473bc4e5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "======= threshold: -0.1 =======\n",
      "Please learn examples and answer questions:\n",
      "\n",
      "Input: Spot can run.\n",
      "output: Spot puede correr.\n",
      "\n",
      "Input: See Spot run.\n",
      "output: Ver correr a Spot.\n",
      "\n",
      "Input: My dog barks.\n",
      "output: Mi perro ladra.\n",
      "\n",
      "Input: Spot can run fast.\n",
      "Output:\n",
      "\n",
      "======= threshold: 0.01 =======\n",
      "Please learn examples and answer questions:\n",
      "\n",
      "Input: Spot can run.\n",
      "output: Spot puede correr.\n",
      "\n",
      "Input: See Spot run.\n",
      "output: Ver correr a Spot.\n",
      "\n",
      "Input: Spot can run fast.\n",
      "Output:\n",
      "\n",
      "======= threshold: 1.0 =======\n",
      "Please learn examples and answer questions:\n",
      "\n",
      "Input: Spot can run fast.\n",
      "Output:\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 自定义 Example Selector"
   ],
   "metadata": {
    "id": "5eHdZwx0Wl0y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "from typing import Dict, List\n",
    "\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples: List, count: int = 2):\n",
    "        self.examples = examples\n",
    "        self.count = count\n",
    "\n",
    "    def add_example(self, example: Dict) -> None:\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables: Dict) -> List:\n",
    "        return random.sample(self.examples, self.count)\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {'input': 'happy', 'output': 'sad'},\n",
    "    {'input': 'tall', 'output': 'short'},\n",
    "    {'input': 'hot', 'output': 'cold'},\n",
    "    {'input': 'fast', 'output': 'slow'},\n",
    "    {'input': 'rich', 'output': 'poor'},\n",
    "]\n",
    "\n",
    "example_selector = RandomExampleSelector(examples, 3)\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    'Input: {input}\\noutput: {output}'\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    prefix='Please learn examples and answer questions:',\n",
    "    example_prompt=example_prompt,\n",
    "    suffix='Input: {input}\\nOutput:',\n",
    "    input_variables=['input']\n",
    ")\n",
    "\n",
    "print(prompt.format(input='sunny'))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kaqBqR-TYo2",
    "outputId": "f2764b7d-fd04-403b-bed6-3690e982c033"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Please learn examples and answer questions:\n",
      "\n",
      "Input: rich\n",
      "output: poor\n",
      "\n",
      "Input: tall\n",
      "output: short\n",
      "\n",
      "Input: fast\n",
      "output: slow\n",
      "\n",
      "Input: sunny\n",
      "Output:\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Output parsers（输出解析器）\n",
    "## CommaSeparatedListOutputParser"
   ],
   "metadata": {
    "id": "qulxdMdAQb9B"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(f'instructions: {instructions}')\n",
    "\n",
    "prompt_tpl = PromptTemplate.from_template(\n",
    "    template='请返回3个最有代表性的{input}.\\n{instructions}',\n",
    "    partial_variables={'instructions': instructions}\n",
    ")\n",
    "\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')\n",
    "prompt = prompt_tpl.format(input='编程语言')\n",
    "\n",
    "output = llm(prompt)\n",
    "print(f'output: {output}, type: {type(output)}')\n",
    "\n",
    "output_format = output_parser.parse(output)\n",
    "print(f'output format: {output_format}, type: {type(output_format)}')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFKEdrTtQcZR",
    "outputId": "e93b65b6-1c04-4994-d610-1b036c6bad6b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "instructions: Your response should be a list of comma separated values, eg: `foo, bar, baz`\n",
      "output: \n",
      "\n",
      "Java, Python, JavaScript, type: <class 'str'>\n",
      "output format: ['Java', 'Python', 'JavaScript'], type: <class 'list'>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "6C87nj7rUg7F"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DatetimeOutputParser"
   ],
   "metadata": {
    "id": "kT4utrv7w3fa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "\n",
    "output_parser = DatetimeOutputParser()\n",
    "instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(f'instructions: {instructions}')\n",
    "\n",
    "prompt_tpl = PromptTemplate.from_template(\n",
    "    template='北京举办奥运会开幕式是哪一年的几点.\\n{instructions}',\n",
    "    partial_variables={'instructions': instructions}\n",
    ")\n",
    "\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')\n",
    "prompt = prompt_tpl.format()\n",
    "\n",
    "output = llm(prompt)\n",
    "print(f'output: {output}, type: {type(output)}')\n",
    "\n",
    "output_format = output_parser.parse(output)\n",
    "print(f'output format: {output_format}, type: {type(output_format)}')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7aZ0z1VfUhcO",
    "outputId": "63290a39-58df-443c-e558-a5a7b450d8a0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "instructions: Write a datetime string that matches the \n",
      "            following pattern: \"%Y-%m-%dT%H:%M:%S.%fZ\". Examples: 369-04-05T20:15:31.537537Z, 1585-09-27T10:20:00.905533Z, 982-11-22T12:53:31.390465Z\n",
      "output: \n",
      "\n",
      "\n",
      "2008-08-08T20:00:00.000000Z, type: <class 'str'>\n",
      "output format: 2008-08-08 20:00:00, type: <class 'datetime.datetime'>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EnumOutputParser"
   ],
   "metadata": {
    "id": "HuEE1uwmw7jK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from enum import Enum\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import EnumOutputParser\n",
    "\n",
    "\n",
    "class Color(Enum):\n",
    "    BLUE = 'blue'\n",
    "    RED = 'red'\n",
    "    GREEN = 'green'\n",
    "\n",
    "\n",
    "output_parser = EnumOutputParser(enum=Color)\n",
    "instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(f'instructions: {instructions}')\n",
    "\n",
    "prompt_tpl = PromptTemplate.from_template(\n",
    "    template='天空是什么颜色？\\n{instructions}',\n",
    "    partial_variables={'instructions': instructions}\n",
    ")\n",
    "\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')\n",
    "prompt = prompt_tpl.format()\n",
    "\n",
    "output = llm(prompt)\n",
    "print(f'output: {output}, type: {type(output)}')\n",
    "\n",
    "output_format = output_parser.parse(output)\n",
    "print(f'output format: {output_format}, type: {type(output_format)}')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCMV-5hwXdJh",
    "outputId": "813bba75-8195-4716-9d4f-748f7bfd8862"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "instructions: Select one of the following options: blue, red, green\n",
      "output: \n",
      "\n",
      "blue, type: <class 'str'>\n",
      "output format: Color.BLUE, type: <enum 'Color'>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 如果值没有在 Color 对象中\n",
    "output_format = output_parser.parse('orange')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "dwqM_rTkZyen",
    "outputId": "2172a781-4d5a-4a00-aa11-75095d396626"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "OutputParserException",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/enum.py\u001B[0m in \u001B[0;36mparse\u001B[0;34m(self, response)\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     28\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.10/enum.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(cls, value, names, module, qualname, type, start)\u001B[0m\n\u001B[1;32m    384\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mnames\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# simple value lookup\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 385\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__new__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    386\u001B[0m         \u001B[0;31m# otherwise, functional API: we're creating a new Enum type\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.10/enum.py\u001B[0m in \u001B[0;36m__new__\u001B[0;34m(cls, value)\u001B[0m\n\u001B[1;32m    709\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mresult\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mexc\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 710\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mve_exc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    711\u001B[0m                 \u001B[0;32melif\u001B[0m \u001B[0mexc\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: 'orange' is not a valid Color",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mOutputParserException\u001B[0m                     Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-99f448cfac35>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0moutput_format\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moutput_parsers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'orange'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/enum.py\u001B[0m in \u001B[0;36mparse\u001B[0;34m(self, response)\u001B[0m\n\u001B[1;32m     27\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 29\u001B[0;31m             raise OutputParserException(\n\u001B[0m\u001B[1;32m     30\u001B[0m                 \u001B[0;34mf\"Response '{response}' is not one of the \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m                 \u001B[0;34mf\"expected values: {self._valid_values}\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOutputParserException\u001B[0m: Response 'orange' is not one of the expected values: ['blue', 'red', 'green']"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XMLOutputParser"
   ],
   "metadata": {
    "id": "3xB71MWNw_vq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import XMLOutputParser\n",
    "\n",
    "output_parser = XMLOutputParser(\n",
    "    tags=['movies', 'movie', 'name', 'director', 'year']\n",
    ")\n",
    "instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(f'instructions: {instructions}')\n",
    "\n",
    "prompt_tpl = PromptTemplate.from_template(\n",
    "    template='请举例2个最有代表性的中国电影，请用中文回答.\\n{instructions}',\n",
    "    partial_variables={'instructions': instructions}\n",
    ")\n",
    "\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')\n",
    "prompt = prompt_tpl.format()\n",
    "\n",
    "output = llm(prompt)\n",
    "print(f'output: {output}, type: {type(output)}')\n",
    "\n",
    "output_format = output_parser.parse(output)\n",
    "print('\\noutput_format: ')\n",
    "for movie in output_format['movies']:\n",
    "\tprint(movie)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUBIX3g9bMad",
    "outputId": "a4c20418-fff2-4dcf-941d-5b3ddec2bc2b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "instructions: The output should be formatted as a XML file.\n",
      "1. Output should conform to the tags below. \n",
      "2. If tags are not given, make them on your own.\n",
      "3. Remember to always open and close all the tags.\n",
      "\n",
      "As an example, for the tags [\"foo\", \"bar\", \"baz\"]:\n",
      "1. String \"<foo>\n",
      "   <bar>\n",
      "      <baz></baz>\n",
      "   </bar>\n",
      "</foo>\" is a well-formatted instance of the schema. \n",
      "2. String \"<foo>\n",
      "   <bar>\n",
      "   </foo>\" is a badly-formatted instance.\n",
      "3. String \"<foo>\n",
      "   <tag>\n",
      "   </tag>\n",
      "</foo>\" is a badly-formatted instance.\n",
      "\n",
      "Here are the output tags:\n",
      "```\n",
      "['movies', 'movie', 'name', 'director', 'year']\n",
      "```\n",
      "output: \n",
      "<movies>\n",
      "    <movie>\n",
      "        <name>霸王别姬</name>\n",
      "        <director>陈凯歌</director>\n",
      "        <year>1993</year>\n",
      "    </movie>\n",
      "    <movie>\n",
      "        <name>活着</name>\n",
      "        <director>张艺谋</director>\n",
      "        <year>1994</year>\n",
      "    </movie>\n",
      "</movies>, type: <class 'str'>\n",
      "\n",
      "output_format: \n",
      "{'movie': [{'name': '霸王别姬'}, {'director': '陈凯歌'}, {'year': '1993'}]}\n",
      "{'movie': [{'name': '活着'}, {'director': '张艺谋'}, {'year': '1994'}]}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## StructuredOutputParser"
   ],
   "metadata": {
    "id": "AeDPMI1gxD4j"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.output_parsers import (\n",
    "    StructuredOutputParser,\n",
    "    ResponseSchema\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name='answer',\n",
    "        description='提问的回答内容。'\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name='source',\n",
    "        description='回答内容的出处网址。'\n",
    "    )\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(\n",
    "    response_schemas)\n",
    "\n",
    "instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(f'instructions: {instructions}')\n",
    "\n",
    "prompt_tpl = PromptTemplate.from_template(\n",
    "    template='请尽可能的回答用户所提的问题。\\n{input}\\n{instructions}',\n",
    "    partial_variables={'instructions': instructions}\n",
    ")\n",
    "\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')\n",
    "prompt = prompt_tpl.format(input='中国有多少个名族？')\n",
    "\n",
    "output = llm(prompt)\n",
    "print(f'output: {output}, type: {type(output)}')\n",
    "\n",
    "output_format = output_parser.parse(output)\n",
    "print('\\noutput_format:')\n",
    "for name, value in output_format.items():\n",
    "    print(name, value)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GecVg3dbjLaF",
    "outputId": "92541125-c418-4d25-8fbf-9df38e85e0db"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "instructions: The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"answer\": string  // 提问的回答内容。\n",
      "\t\"source\": string  // 回答内容的出处网址。\n",
      "}\n",
      "```\n",
      "output: \n",
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"answer\": \"中国有56个民族。\",\n",
      "\t\"source\": \"https://baike.baidu.com/item/%E4%B8%AD%E5%9B%BD%E6%B0%91%E6%97%8F/111037?fr=aladdin\"\n",
      "}\n",
      "```\n",
      ", type: <class 'str'>\n",
      "\n",
      "output_format:\n",
      "answer 中国有56个民族。\n",
      "source https://baike.baidu.com/item/%E4%B8%AD%E5%9B%BD%E6%B0%91%E6%97%8F/111037?fr=aladdin\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PydanticOutputParser"
   ],
   "metadata": {
    "id": "tf9wIZkoxJ0b"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install BeautifulSoup4"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_ijds4FdxOo",
    "outputId": "99164d25-0dcb-459f-9d80-b3fa87699ceb"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: BeautifulSoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from BeautifulSoup4) (2.5)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "class MovieInfo(BaseModel):\n",
    "    name: str = Field(description='电影名称')\n",
    "    director: str = Field(description='电影导演')\n",
    "    scriptwriter: str = Field(description='电影编剧')\n",
    "    language: str = Field(description='电影语言')\n",
    "    release_date: str = Field(description='电影上映日期')\n",
    "    movie_type: str = Field(description='电影类型')\n",
    "    rating: float = Field(description=\"电影评分\")\n",
    "    length: str = Field(description='电影片长')\n",
    "\n",
    "\n",
    "def get_movie_html(url):\n",
    "    \"\"\"解析并拼接关系数据的html\"\"\"\n",
    "    response = requests.get(\n",
    "        url, headers={\n",
    "            'User-Agent': (\n",
    "                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                'Chrome/116.0.0.0 '\n",
    "                'Safari/537.36'\n",
    "            )}\n",
    "    )\n",
    "    html_markup = ''\n",
    "    if response.status_code == 200:\n",
    "        html_markup = response.text\n",
    "        soup = BeautifulSoup(html_markup, 'html.parser')\n",
    "        html_markup = str(soup.h1)\n",
    "\n",
    "        element = soup.find(id='info')\n",
    "        html_markup += str(element)\n",
    "\n",
    "        element = soup.find(id='interest_sectl')\n",
    "        html_markup += str(element)\n",
    "\n",
    "    return html_markup\n",
    "\n",
    "\n",
    "def chatgpt_parse(html):\n",
    "    \"\"\"解析内容\"\"\"\n",
    "    parser = PydanticOutputParser(pydantic_object=MovieInfo)\n",
    "\n",
    "    messages = [HumanMessagePromptTemplate.from_template(\n",
    "        template=('从以下HTML中提取电影信息:\\n{html}.\\n'\n",
    "                  '{format_instructions}\\n'),\n",
    "    )]\n",
    "    prompt_tpl = ChatPromptTemplate(messages=messages).format_prompt(\n",
    "        html=html,\n",
    "        format_instructions=parser.get_format_instructions()\n",
    "    )\n",
    "    model = ChatOpenAI(temperature=0.0)\n",
    "    _output = model(prompt_tpl.to_messages())\n",
    "    recipe = parser.parse(_output.content)\n",
    "    return recipe\n",
    "\n",
    "\n",
    "movie_url = 'https://movie.douban.com/subject/1292063/'\n",
    "html_str = get_movie_html(movie_url)\n",
    "\n",
    "data = chatgpt_parse(html_str)\n",
    "\n",
    "print(f'type: {type(data)}')\n",
    "for key, value in data.dict().items():\n",
    "    print(f'{key}: {value}')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCYECyK0kGTS",
    "outputId": "ca7c682b-c433-49db-c3cf-94634d23f525"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "type: <class '__main__.MovieInfo'>\n",
      "name: 美丽人生 La vita è bella\n",
      "director: 罗伯托·贝尼尼\n",
      "scriptwriter: 温琴佐·切拉米 / 罗伯托·贝尼尼\n",
      "language: 意大利语 / 德语 / 英语\n",
      "release_date: 2020-01-03(中国大陆) / 1997-12-20(意大利)\n",
      "movie_type: 剧情 / 喜剧 / 爱情 / 战争\n",
      "rating: 9.6\n",
      "length: 116分钟(国际版) / 125分钟\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## OutputFixingParser"
   ],
   "metadata": {
    "id": "c269lBLJxz1h"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.schema.output_parser import OutputParserException\n",
    "from langchain.output_parsers import (\n",
    "    PydanticOutputParser,\n",
    "    OutputFixingParser,\n",
    ")\n",
    "\n",
    "\n",
    "class MovieInfo(BaseModel):\n",
    "    name: str = Field(description='电影名称')\n",
    "    director: str = Field(description='电影导演')\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=MovieInfo)\n",
    "error_info = \"{'name': '美丽人生', 'director': '罗伯托·贝尼尼'}\"\n",
    "\n",
    "try:\n",
    "\n",
    "    parser.parse(error_info)\n",
    "except OutputParserException as e:\n",
    "    print(e)\n",
    "    fix_parser = OutputFixingParser.from_llm(\n",
    "        parser=parser, llm=OpenAI()\n",
    "    )\n",
    "    print(fix_parser.parse(error_info))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VaEt600SpaR9",
    "outputId": "74fb8835-cdc6-4608-d546-b5e84f60a413"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Failed to parse MovieInfo from completion {'name': '美丽人生', 'director': '罗伯托·贝尼尼'}. Got: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "name='美丽人生' director='罗伯托·贝尼尼'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RetryWithErrorOutputParser"
   ],
   "metadata": {
    "id": "PiN0x9fxx29Z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.schema.output_parser import OutputParserException\n",
    "from langchain.output_parsers import (\n",
    "    PydanticOutputParser,\n",
    "    RetryWithErrorOutputParser,\n",
    ")\n",
    "\n",
    "\n",
    "class Action(BaseModel):\n",
    "    action: str = Field(description='要执行的动作')\n",
    "    action_input: str = Field(description='动作的输入')\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Action)\n",
    "prompt = PromptTemplate(\n",
    "    template='请回答用户的问题.\\n{instructions}\\n{query}\\n',\n",
    "    input_variables=['query'],\n",
    "    partial_variables={\n",
    "        'instructions': parser.get_format_instructions()\n",
    "    },\n",
    ")\n",
    "\n",
    "prompt_value = prompt.format_prompt(query='西游记的作者是谁？')\n",
    "bad_response = '{\"action\": \"search\"}'\n",
    "\n",
    "try:\n",
    "    parser.parse(bad_response)\n",
    "except OutputParserException as e:\n",
    "    retry_parser = RetryWithErrorOutputParser.from_llm(\n",
    "        parser=parser, llm=OpenAI(temperature=0)\n",
    "    )\n",
    "    print(retry_parser.parse_with_prompt(bad_response, prompt_value))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zT_P2gq74vH8",
    "outputId": "8e1ce3a7-02e8-4867-e44d-a27e1bb3d2e2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "action='search' action_input='西游记的作者是谁？'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 自定义Output Parser"
   ],
   "metadata": {
    "id": "RJa93QK2x6Oz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import BaseOutputParser\n",
    "\n",
    "\n",
    "class UMLOutputParser(BaseOutputParser):\n",
    "    def get_format_instructions(self):\n",
    "        return (\n",
    "            'The output should be a markdown code snippet '\n",
    "            'formatted in the following schema, '\n",
    "            'including the leading and trailing \"```uml\" and \"```\":\\n'\n",
    "            '```uml\\n'\n",
    "            '@startuml\\n'\n",
    "            '......\\n'\n",
    "            '@enduml\\n'\n",
    "            '```'\n",
    "        )\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        match = re.search(r\"```(uml)?(.*)```\", text, re.DOTALL)\n",
    "        uml_str = match.group(2).strip()\n",
    "        return uml_str\n",
    "\n",
    "    @property\n",
    "    def _type(self):\n",
    "        return 'uml'\n",
    "\n",
    "\n",
    "output_parser = UMLOutputParser()\n",
    "instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt_tpl = PromptTemplate.from_template(\n",
    "    template='请画一张{content}。\\n{instructions}',\n",
    "    partial_variables={'instructions': instructions}\n",
    ")\n",
    "\n",
    "prompt = prompt_tpl.format(content='采购审批流程图')\n",
    "\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')\n",
    "output = llm(prompt)\n",
    "\n",
    "output_format = output_parser.parse(output)\n",
    "print(output_format)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYfX5BsjzUK-",
    "outputId": "e193e6d9-f0ac-4c9d-9763-a1fe8ea05d74"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "@startuml\n",
      "title 采购审批流程图\n",
      "\n",
      "start\n",
      ":提交采购申请;\n",
      ":部门经理审批;\n",
      "if (金额 <= 5000) then (是)\n",
      "  :审批通过;\n",
      "  else (否)\n",
      "  :提交给总经理;\n",
      "endif\n",
      ":总经理审批;\n",
      "if (金额 <= 10000) then (是)\n",
      "  :审批通过;\n",
      "  else (否)\n",
      "  :提交给财务部;\n",
      "endif\n",
      ":财务部审批;\n",
      "if (金额 <= 20000) then (是)\n",
      "  :审批通过;\n",
      "  else (否)\n",
      "  :提交给采购部;\n",
      "endif\n",
      ":采购部审批;\n",
      ":生成采购订单;\n",
      ":采购商品;\n",
      ":收到商品并验收;\n",
      ":付款;\n",
      "stop\n",
      "@enduml\n"
     ]
    }
   ]
  }
 ]
}
